{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772c7fd8",
   "metadata": {},
   "source": [
    "## 1. 20뉴스그룹 데이터 전처리\n",
    "- https://wikidocs.net/69141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a132bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9645e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "# 하나의 샘플에 최소 단어 2개는 있어야 함. \n",
    "# 그래야만 중심 단어, 주변 단어의 관계가 성립하며 그렇지 않으면 샘플을 구성할 수 없어 에러가 발생\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0db248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/bm3zqtjx7xsb__yxj254h7kh0000gn/T/ipykernel_49700/4080772658.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf712e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist will speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet with orange micros grappler syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument with murphy scared hell when came las...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure about story seem biased what disagre...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize that principle your strongest...  \n",
       "3      notwithstanding legitimate fuss about this pro...  \n",
       "4      well will have change scoring playoff pool unf...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist will speak...  \n",
       "11310                                                     \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet with orange micros grappler syste...  \n",
       "11313  argument with murphy scared hell when came las...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76fcecf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈값이 있는지 확인\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ed4cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10995\n"
     ]
    }
   ],
   "source": [
    "# null인 값 삭제 \n",
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :',len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6210a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어를 제거, 모든 샘플 중 단어가 1개 이하인 경우도 삭제 \n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeba29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "# 정수 인코딩 \n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b67e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9,\n",
       "  59,\n",
       "  603,\n",
       "  207,\n",
       "  3278,\n",
       "  1495,\n",
       "  474,\n",
       "  702,\n",
       "  9470,\n",
       "  13686,\n",
       "  5533,\n",
       "  15229,\n",
       "  702,\n",
       "  442,\n",
       "  702,\n",
       "  70,\n",
       "  1148,\n",
       "  1095,\n",
       "  1036,\n",
       "  20294,\n",
       "  984,\n",
       "  705,\n",
       "  4295,\n",
       "  702,\n",
       "  217,\n",
       "  207,\n",
       "  1979,\n",
       "  15230,\n",
       "  13686,\n",
       "  4865,\n",
       "  4520,\n",
       "  87,\n",
       "  1530,\n",
       "  6,\n",
       "  52,\n",
       "  149,\n",
       "  581,\n",
       "  661,\n",
       "  4406,\n",
       "  4988,\n",
       "  4866,\n",
       "  1920,\n",
       "  755,\n",
       "  10668,\n",
       "  1103,\n",
       "  7838,\n",
       "  442,\n",
       "  957,\n",
       "  10669,\n",
       "  634,\n",
       "  51,\n",
       "  228,\n",
       "  2669,\n",
       "  4989,\n",
       "  178,\n",
       "  66,\n",
       "  222,\n",
       "  4521,\n",
       "  6066,\n",
       "  68,\n",
       "  4296],\n",
       " [1027,\n",
       "  532,\n",
       "  2,\n",
       "  60,\n",
       "  98,\n",
       "  582,\n",
       "  107,\n",
       "  800,\n",
       "  23,\n",
       "  79,\n",
       "  4522,\n",
       "  333,\n",
       "  7839,\n",
       "  864,\n",
       "  421,\n",
       "  3825,\n",
       "  458,\n",
       "  6488,\n",
       "  458,\n",
       "  2700,\n",
       "  4730,\n",
       "  333,\n",
       "  23,\n",
       "  9,\n",
       "  4731,\n",
       "  7263,\n",
       "  186,\n",
       "  310,\n",
       "  146,\n",
       "  170,\n",
       "  642,\n",
       "  1260,\n",
       "  107,\n",
       "  33571,\n",
       "  13,\n",
       "  985,\n",
       "  33572,\n",
       "  33573,\n",
       "  9471,\n",
       "  11491]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34ba5816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 64282\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1 \n",
    "print('단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d69df",
   "metadata": {},
   "source": [
    "## 2. 네거티브 샘플링을 통한 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbbf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "# 네거티브 샘플링\n",
    "# 10개만 해보자 \n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba63352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(world (70), described (984)) -> 1\n",
      "(look (66), dominican (35851)) -> 0\n",
      "(commited (7838), mcovingt (12663)) -> 0\n",
      "(media (702), incidences (20294)) -> 1\n",
      "(reports (755), reason (149)) -> 1\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          idx2word[pairs[i][0]], pairs[i][0], \n",
    "          idx2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93a87939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :',len(skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85eb6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c254b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 뉴스그룹 샘풀에 대해 수행\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb3716",
   "metadata": {},
   "source": [
    "## 3. Skip-Gram with Negative Sampling(SGNS) 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9781397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "977a6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터인 임베딩 벡터의 차원은 100으로 설정하고 2개의 임베딩 층 추가 ㅠㅠ\n",
    "embedding_dim = 100\n",
    "\n",
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b235adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       6428200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       6428200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1)            0           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,856,400\n",
      "Trainable params: 12,856,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61996aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skip_grams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mskip_grams\u001b[49m):\n\u001b[1;32m      4\u001b[0m         first_elem \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39melem[\u001b[38;5;241m0\u001b[39m]))[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m         second_elem \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39melem[\u001b[38;5;241m0\u001b[39m]))[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skip_grams' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 2):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be62adb",
   "metadata": {},
   "source": [
    "## 4. 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "# 모델 로드\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b643a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad68b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5af549",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['knife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['engine'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
